{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('eucalc_directory')\n",
    "import eucalc as ec\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from numpy import genfromtxt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "datafolder = \"Old_Young_Comparison\"\n",
    "\n",
    "all_files = os.listdir(datafolder)\n",
    "\n",
    "# Get all CSV filenames in the folder\n",
    "names = [file for file in all_files if file.lower().endswith(('.tif', '.tiff'))]\n",
    "\n",
    "\n",
    "# Split names into K8 and K14 groups\n",
    "names_k8 = [nm for nm in names if 'K8' in nm]\n",
    "names_k14 = [nm for nm in names if 'K14' in nm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampeuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tifffile as tiff\n",
    "import eucalc as ec\n",
    "import matplotlib.colors as mcolors\n",
    "from PIL import Image\n",
    "class EctImg:\n",
    "    def __init__(self, nm, img, k=20, xinterval=(-1., 1.), xpoints=100, yinterval=(-1., 1.), ypoints=100):\n",
    "        self.xinterval = xinterval\n",
    "        self.yinterval = yinterval\n",
    "        self.xpoints = xpoints\n",
    "        self.ypoints = ypoints\n",
    "        self.image = self.compute(img, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        self.nm = nm\n",
    "    def compute(self, img, k, xinterval, xpoints, yinterval, ypoints):\n",
    "        cplx = ec.EmbeddedComplex(img)\n",
    "        cplx.preproc_ect()\n",
    "        thetas = np.random.uniform(0, 2 * np.pi, k + 1)\n",
    "        ect1 = np.empty((k, xpoints), dtype=float)\n",
    "        for i in range(k):\n",
    "            theta = thetas[i]\n",
    "            direction = np.array((np.sin(theta), np.cos(theta)))\n",
    "            ect_dir = cplx.compute_euler_characteristic_transform(direction)\n",
    "            T = np.linspace(xinterval[0], xinterval[1], xpoints)\n",
    "            ect1[i] = [ect_dir.evaluate(t) for t in T]\n",
    "        return ect1\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        # Using xinterval and yinterval directly in extent\n",
    "        plt.imshow(self.image, aspect='auto', extent=[self.xinterval[0], self.xinterval[1], self.yinterval[0], self.yinterval[1]], origin='lower', interpolation='none')\n",
    "        plt.colorbar(label='Density')\n",
    "        plt.xlabel('X-axis')\n",
    "        plt.ylabel('Y-axis')\n",
    "        plt.title('ECT Image Plot for '+ self.nm)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def compute_ExIm(names, datafolder, k=480, xinterval=(-1.5, 1.5), xpoints=300, yinterval=(-450., 50.), ypoints=500):\n",
    "    ExImage = []\n",
    "    \n",
    "    for nm in names: \n",
    "        file_path = os.path.join(datafolder, nm)\n",
    "        with Image.open(file_path) as img:\n",
    "            img_array = np.array(img)\n",
    "        \n",
    "        # Now, compute the ECT image using the new array\n",
    "        ect = EctImg(nm, img_array, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        exim = ect.compute(img_array, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        ExImage.append(exim)\n",
    "\n",
    "    return ExImage\n",
    "\n",
    "exims = compute_ExIm(names_k8, datafolder,k=360, xinterval=(-1.5, 1.5), xpoints=75, yinterval=(-20, 60), ypoints=80)\n",
    "flattened_k8 = [image.flatten() for image in exims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tifffile as tiff\n",
    "import eucalc as ec\n",
    "import matplotlib.colors as mcolors\n",
    "from PIL import Image\n",
    "class EctImg:\n",
    "    def __init__(self, nm, img, k=20, xinterval=(-1., 1.), xpoints=100, yinterval=(-1., 1.), ypoints=100):\n",
    "        self.xinterval = xinterval\n",
    "        self.yinterval = yinterval\n",
    "        self.xpoints = xpoints\n",
    "        self.ypoints = ypoints\n",
    "        self.image = self.compute(img, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        self.nm = nm\n",
    "    def compute(self, img, k, xinterval, xpoints, yinterval, ypoints):\n",
    "        cplx = ec.EmbeddedComplex(img)\n",
    "        cplx.preproc_ect()\n",
    "        thetas = np.linspace(0, 2 * np.pi, k + 1)\n",
    "        ect1 = np.empty((k, xpoints), dtype=float)\n",
    "        for i in range(k):\n",
    "            theta = thetas[i]\n",
    "            direction = np.array((np.sin(theta), np.cos(theta)))\n",
    "            ect_dir = cplx.compute_euler_characteristic_transform(direction)\n",
    "            T = np.linspace(xinterval[0], xinterval[1], xpoints)\n",
    "            ect1[i] = [ect_dir.evaluate(t) for t in T]\n",
    "        return ect1\n",
    "\n",
    "\n",
    "def compute_ExIm(names, datafolder, k=480, xinterval=(-1.5, 1.5), xpoints=300, yinterval=(-450., 50.), ypoints=500):\n",
    "    ExImage = []\n",
    "    \n",
    "    for nm in names: \n",
    "        file_path = os.path.join(datafolder, nm)\n",
    "        with Image.open(file_path) as img:\n",
    "            img_array = np.array(img)\n",
    "        \n",
    "        # Now, compute the ECT image using the new array\n",
    "        ect = EctImg(nm, img_array, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        exim = ect.compute(img_array, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        ExImage.append(exim)\n",
    "\n",
    "    return ExImage\n",
    "\n",
    "exims = compute_ExIm(names_k8, datafolder,k=360, xinterval=(-1.5, 1.5), xpoints=150, yinterval=(-20, 60), ypoints=80)\n",
    "flattened_k8 = [image.flatten() for image in exims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyang/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Average accuracy for age O is 0.6857142857142857\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "def extract_age(name):\n",
    "    return name[0]\n",
    "\n",
    "# Assume names_k8 and flattened_k8 are already defined\n",
    "ages_k8 = [extract_age(name) for name in names_k8]\n",
    "X = np.array(flattened_k8)\n",
    "y = np.array(ages_k8)\n",
    "unique_ages = np.unique(y)\n",
    "ypoints, xpoints = 110, 300  # Set the appropriate image dimensions\n",
    "\n",
    "# Number of trials for train-test splitting\n",
    "num_trials = 50\n",
    "age = unique_ages[0]\n",
    "print(f\"Processing age group: {age}\")\n",
    "\n",
    "# Binary classification: 1 if the sample belongs to the current age, 0 otherwise\n",
    "binary_labels = np.where(y == age, 1, 0)\n",
    "\n",
    "# Parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 500, 1000],   # Number of trees in the forest\n",
    "    'max_depth': [3, 5, 7, 20, None],             # Tree depth (None means no limit)\n",
    "    'min_samples_split': [2, 3, 5, 10],           # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform Grid Search to get the best parameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=40),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X, binary_labels)\n",
    "\n",
    "# Extract best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters for age group {age}: {best_params}\")\n",
    "\n",
    "scores = []\n",
    "# Loop for multiple random splits\n",
    "for trial in range(num_trials):\n",
    "    # Randomize train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, binary_labels, test_size=0.2, random_state=40+trial)\n",
    "\n",
    "    # Initialize and train the Random Forest classifier using best_params\n",
    "    rf_classifier = RandomForestClassifier(random_state=40, **best_params)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    score = rf_classifier.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "    \n",
    "print(f'Average accuracy for age {age} is {np.mean(scores)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DETECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def process_ect_image(ect_image, xinterval, xpoints):\n",
    "    \"\"\"\n",
    "    Process a 2D ECT image by:\n",
    "      1. Averaging across curves (axis=0) to yield a mean curve.\n",
    "      2. Integrating the mean curve using a cumulative sum.\n",
    "      3. Subtracting the average (mean) value from the integrated curve for baseline correction.\n",
    "    \n",
    "    Parameters:\n",
    "      ect_image : numpy.ndarray\n",
    "          2D array with shape (k, xpoints), where each row is one curve.\n",
    "      xinterval : tuple of float\n",
    "          The (min, max) values of the x-axis, e.g., (-1.5, 1.5).\n",
    "      xpoints : int\n",
    "          Number of points along the x-direction.\n",
    "          \n",
    "    Returns:\n",
    "      integrated_curve_centered : numpy.ndarray\n",
    "          1D processed curve with the same length as the number of xpoints.\n",
    "    \"\"\"\n",
    "    # Step 1: Average across all curves (rows) to get one representative curve.\n",
    "    mean_curve = np.mean(ect_image, axis=0)\n",
    "    \n",
    "    # Compute spacing (assuming uniform distribution in the given xinterval)\n",
    "    dx = (xinterval[1] - xinterval[0]) / (xpoints - 1)\n",
    "    \n",
    "    # Step 2: Numerically integrate the mean curve using a cumulative sum.\n",
    "    # The cumulative integration approximates the integral from the start to each x point.\n",
    "    integrated_curve = np.cumsum(mean_curve) * dx\n",
    "    \n",
    "    # Step 3: Subtract the average value of the integrated curve (baseline correction)\n",
    "    baseline = np.mean(integrated_curve)\n",
    "    integrated_curve_centered = integrated_curve - baseline\n",
    "    \n",
    "    return integrated_curve_centered\n",
    "\n",
    "# Example usage after computing your ECT images (exims)\n",
    "# Assume exims is a list of 2D arrays from your compute_ExIm function,\n",
    "# and the x-interval and number of x-points correspond to how you computed them.\n",
    "xinterval = (-1.5, 1.5)\n",
    "xpoints = 75\n",
    "\n",
    "# Process each sample (ECT image)\n",
    "processed_samples = [process_ect_image(ect_image, xinterval, xpoints) for ect_image in exims]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 200}\n",
      "Average accuracy for age O is 0.7171428571428571\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "def extract_age(name):\n",
    "    return name[0]\n",
    "\n",
    "# Assume names_k8 and flattened_k8 are already defined\n",
    "ages_k8 = [extract_age(name) for name in names_k8]\n",
    "X = np.array(processed_samples)\n",
    "y = np.array(ages_k8)\n",
    "unique_ages = np.unique(y)\n",
    "ypoints, xpoints = 110, 300  # Set the appropriate image dimensions\n",
    "\n",
    "# Number of trials for train-test splitting\n",
    "num_trials = 50\n",
    "age = unique_ages[0]\n",
    "print(f\"Processing age group: {age}\")\n",
    "\n",
    "# Binary classification: 1 if the sample belongs to the current age, 0 otherwise\n",
    "binary_labels = np.where(y == age, 1, 0)\n",
    "\n",
    "# Parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 500, 1000],   # Number of trees in the forest\n",
    "    'max_depth': [3, 5, 7, 20, None],             # Tree depth (None means no limit)\n",
    "    'min_samples_split': [2, 3, 5, 10],           # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform Grid Search to get the best parameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=40),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X, binary_labels)\n",
    "\n",
    "# Extract best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters for age group {age}: {best_params}\")\n",
    "\n",
    "scores = []\n",
    "# Loop for multiple random splits\n",
    "for trial in range(num_trials):\n",
    "    # Randomize train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, binary_labels, test_size=0.2, random_state=40+trial)\n",
    "\n",
    "    # Initialize and train the Random Forest classifier using best_params\n",
    "    rf_classifier = RandomForestClassifier(random_state=40, **best_params)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    score = rf_classifier.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "    \n",
    "print(f'Average accuracy for age {age} is {np.mean(scores)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampeuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tifffile as tiff\n",
    "import eucalc as ec\n",
    "import matplotlib.colors as mcolors\n",
    "from PIL import Image\n",
    "class EctImg:\n",
    "    def __init__(self, nm, img, k=20, xinterval=(-1., 1.), xpoints=100, yinterval=(-1., 1.), ypoints=100):\n",
    "        self.xinterval = xinterval\n",
    "        self.yinterval = yinterval\n",
    "        self.xpoints = xpoints\n",
    "        self.ypoints = ypoints\n",
    "        self.image = self.compute(img, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        self.nm = nm\n",
    "    def compute(self, img, k, xinterval, xpoints, yinterval, ypoints):\n",
    "        cplx = ec.EmbeddedComplex(img)\n",
    "        cplx.preproc_ect()\n",
    "        thetas = np.random.uniform(0, 2 * np.pi, k + 1)\n",
    "        ect1 = np.empty((k, xpoints), dtype=float)\n",
    "        for i in range(k):\n",
    "            theta = thetas[i]\n",
    "            direction = np.array((np.sin(theta), np.cos(theta)))\n",
    "            ect_dir = cplx.compute_euler_characteristic_transform(direction)\n",
    "            T = np.linspace(xinterval[0], xinterval[1], xpoints)\n",
    "            ect1[i] = [ect_dir.evaluate(t) for t in T]\n",
    "        return ect1\n",
    "\n",
    "\n",
    "def compute_ExIm(names, datafolder, k=480, xinterval=(-1.5, 1.5), xpoints=300, yinterval=(-450., 50.), ypoints=500):\n",
    "    ExImage = []\n",
    "    \n",
    "    for nm in names: \n",
    "        file_path = os.path.join(datafolder, nm)\n",
    "        with Image.open(file_path) as img:\n",
    "            img_array = np.array(img)\n",
    "        \n",
    "        # Now, compute the ECT image using the new array\n",
    "        ect = EctImg(nm, img_array, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        exim = ect.compute(img_array, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        ExImage.append(exim)\n",
    "\n",
    "    return ExImage\n",
    "\n",
    "exims = compute_ExIm(names_k14, datafolder,k=360, xinterval=(-1.5, 1.5), xpoints=75, yinterval=(-20, 60), ypoints=80)\n",
    "flattened_k14 = [image.flatten() for image in exims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tifffile as tiff\n",
    "import eucalc as ec\n",
    "import matplotlib.colors as mcolors\n",
    "from PIL import Image\n",
    "class EctImg:\n",
    "    def __init__(self, nm, img, k=20, xinterval=(-1., 1.), xpoints=100, yinterval=(-1., 1.), ypoints=100):\n",
    "        self.xinterval = xinterval\n",
    "        self.yinterval = yinterval\n",
    "        self.xpoints = xpoints\n",
    "        self.ypoints = ypoints\n",
    "        self.image = self.compute(img, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        self.nm = nm\n",
    "    def compute(self, img, k, xinterval, xpoints, yinterval, ypoints):\n",
    "        cplx = ec.EmbeddedComplex(img)\n",
    "        cplx.preproc_ect()\n",
    "        thetas = np.linspace(0, 2 * np.pi, k + 1)\n",
    "        ect1 = np.empty((k, xpoints), dtype=float)\n",
    "        for i in range(k):\n",
    "            theta = thetas[i]\n",
    "            direction = np.array((np.sin(theta), np.cos(theta)))\n",
    "            ect_dir = cplx.compute_euler_characteristic_transform(direction)\n",
    "            T = np.linspace(xinterval[0], xinterval[1], xpoints)\n",
    "            ect1[i] = [ect_dir.evaluate(t) for t in T]\n",
    "        return ect1\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        # Using xinterval and yinterval directly in extent\n",
    "        plt.imshow(self.image, aspect='auto', extent=[self.xinterval[0], self.xinterval[1], self.yinterval[0], self.yinterval[1]], origin='lower', interpolation='none')\n",
    "        plt.colorbar(label='Density')\n",
    "        plt.xlabel('X-axis')\n",
    "        plt.ylabel('Y-axis')\n",
    "        plt.title('ECT Image Plot for '+ self.nm)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def compute_ExIm(names, datafolder, k=480, xinterval=(-1.5, 1.5), xpoints=300, yinterval=(-450., 50.), ypoints=500):\n",
    "    ExImage = []\n",
    "    \n",
    "    for nm in names: \n",
    "        file_path = os.path.join(datafolder, nm)\n",
    "        with Image.open(file_path) as img:\n",
    "            img_array = np.array(img)\n",
    "        \n",
    "        # Now, compute the ECT image using the new array\n",
    "        ect = EctImg(nm, img_array, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        exim = ect.compute(img_array, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        ExImage.append(exim)\n",
    "\n",
    "    return ExImage\n",
    "\n",
    "exims = compute_ExIm(names_k14, datafolder,k=360, xinterval=(-1.5, 1.5), xpoints=300, yinterval=(-20, 60), ypoints=80)\n",
    "flattened_k14 = [image.flatten() for image in exims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Average accuracy for loop 0 is 0.6871428571428573\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 1000}\n",
      "Average accuracy for loop 1 is 0.6971428571428572\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 2 is 0.6928571428571427\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Average accuracy for loop 3 is 0.69\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Average accuracy for loop 4 is 0.6985714285714286\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Average accuracy for loop 5 is 0.7014285714285714\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 1000}\n",
      "Average accuracy for loop 6 is 0.6914285714285714\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 500}\n",
      "Average accuracy for loop 7 is 0.7042857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Average accuracy for loop 8 is 0.6914285714285715\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Average accuracy for loop 9 is 0.6714285714285714\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 10 is 0.7157142857142857\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Average accuracy for loop 11 is 0.6985714285714286\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Average accuracy for loop 12 is 0.7228571428571429\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Average accuracy for loop 13 is 0.7071428571428571\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 1000}\n",
      "Average accuracy for loop 14 is 0.6914285714285715\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Average accuracy for loop 15 is 0.6900000000000002\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Average accuracy for loop 16 is 0.6957142857142857\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 17 is 0.6985714285714285\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Average accuracy for loop 18 is 0.6971428571428572\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 500}\n",
      "Average accuracy for loop 19 is 0.692857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Average accuracy for loop 20 is 0.702857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Average accuracy for loop 21 is 0.6957142857142857\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 22 is 0.6828571428571429\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100}\n",
      "Average accuracy for loop 23 is 0.7057142857142857\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 24 is 0.7057142857142857\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 25 is 0.6942857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Average accuracy for loop 26 is 0.692857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Average accuracy for loop 27 is 0.7085714285714286\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 28 is 0.69\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Average accuracy for loop 29 is 0.712857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Average accuracy for loop 30 is 0.712857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 50}\n",
      "Average accuracy for loop 31 is 0.6914285714285714\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 32 is 0.7099999999999999\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Average accuracy for loop 33 is 0.6842857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Average accuracy for loop 34 is 0.6742857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "Average accuracy for loop 35 is 0.7085714285714285\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Average accuracy for loop 36 is 0.6957142857142858\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 37 is 0.692857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 38 is 0.6785714285714285\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 39 is 0.68\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Average accuracy for loop 40 is 0.69\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Average accuracy for loop 41 is 0.6942857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 42 is 0.69\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Average accuracy for loop 43 is 0.6985714285714286\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Average accuracy for loop 44 is 0.7042857142857143\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Average accuracy for loop 45 is 0.6828571428571429\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Average accuracy for loop 46 is 0.7014285714285714\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 47 is 0.6971428571428572\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Average accuracy for loop 48 is 0.6985714285714286\n",
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Average accuracy for loop 49 is 0.6985714285714286\n",
      "Maximum accuracy is 0.7228571428571429\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "def extract_age(name):\n",
    "    return name[0]\n",
    "\n",
    "import os\n",
    "import tifffile as tiff\n",
    "import eucalc as ec\n",
    "import matplotlib.colors as mcolors\n",
    "from PIL import Image\n",
    "class EctImg:\n",
    "    def __init__(self, nm, img, k=20, xinterval=(-1., 1.), xpoints=100, yinterval=(-1., 1.), ypoints=100):\n",
    "        self.xinterval = xinterval\n",
    "        self.yinterval = yinterval\n",
    "        self.xpoints = xpoints\n",
    "        self.ypoints = ypoints\n",
    "        self.image = self.compute(img, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        self.nm = nm\n",
    "    def compute(self, img, k, xinterval, xpoints, yinterval, ypoints):\n",
    "        cplx = ec.EmbeddedComplex(img)\n",
    "        cplx.preproc_ect()\n",
    "        thetas = np.random.uniform(0, 2 * np.pi, k + 1)\n",
    "        ect1 = np.empty((k, xpoints), dtype=float)\n",
    "        for i in range(k):\n",
    "            theta = thetas[i]\n",
    "            direction = np.array((np.sin(theta), np.cos(theta)))\n",
    "            ect_dir = cplx.compute_euler_characteristic_transform(direction)\n",
    "            T = np.linspace(xinterval[0], xinterval[1], xpoints)\n",
    "            ect1[i] = [ect_dir.evaluate(t) for t in T]\n",
    "        return ect1\n",
    "\n",
    "\n",
    "def compute_ExIm(names, datafolder, k=480, xinterval=(-1.5, 1.5), xpoints=300, yinterval=(-450., 50.), ypoints=500):\n",
    "    ExImage = []\n",
    "    \n",
    "    for nm in names: \n",
    "        file_path = os.path.join(datafolder, nm)\n",
    "        with Image.open(file_path) as img:\n",
    "            img_array = np.array(img)\n",
    "        \n",
    "        # Now, compute the ECT image using the new array\n",
    "        ect = EctImg(nm, img_array, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        exim = ect.compute(img_array, k, xinterval, xpoints, yinterval, ypoints)\n",
    "        ExImage.append(exim)\n",
    "\n",
    "    return ExImage\n",
    "max_score = 0\n",
    "for i in range(50):\n",
    "    exims = compute_ExIm(names_k14, datafolder,k=200, xinterval=(-1.5, 1.5), xpoints=75, yinterval=(-20, 60), ypoints=80)\n",
    "    flattened_k14 = [image.flatten() for image in exims]\n",
    "    \n",
    "    # Assume names_k8 and flattened_k8 are already defined\n",
    "    ages_k14 = [extract_age(name) for name in names_k14]\n",
    "    X = np.array(flattened_k14)\n",
    "    y = np.array(ages_k14)\n",
    "    unique_ages = np.unique(y)\n",
    "    ypoints, xpoints = 110, 300  # Set the appropriate image dimensions\n",
    "\n",
    "    # Number of trials for train-test splitting\n",
    "    num_trials = 50\n",
    "    age = unique_ages[0]\n",
    "    print(f\"Processing age group: {age}\")\n",
    "\n",
    "    # Binary classification: 1 if the sample belongs to the current age, 0 otherwise\n",
    "    binary_labels = np.where(y == age, 1, 0)\n",
    "\n",
    "    # Parameter grid for Random Forest\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 500, 1000],   # Number of trees in the forest\n",
    "        'max_depth': [3, 5, 7, 20, None],             # Tree depth (None means no limit)\n",
    "        'min_samples_split': [2, 3, 5, 10],           # Minimum samples required to split a node\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    # Perform Grid Search to get the best parameters\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(random_state=40),\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=3,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_search.fit(X, binary_labels)\n",
    "\n",
    "    # Extract best parameters from the grid search\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best parameters for age group {age}: {best_params}\")\n",
    "\n",
    "    scores = []\n",
    "    # Loop for multiple random splits\n",
    "    for trial in range(num_trials):\n",
    "        # Randomize train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, binary_labels, test_size=0.2, random_state=40+trial)\n",
    "\n",
    "        # Initialize and train the Random Forest classifier using best_params\n",
    "        rf_classifier = RandomForestClassifier(random_state=40, **best_params)\n",
    "        rf_classifier.fit(X_train, y_train)\n",
    "        score = rf_classifier.score(X_test, y_test)\n",
    "        scores.append(score)\n",
    "    max_score = max(max_score, np.mean(scores))\n",
    "    print(f'Average accuracy for loop {i} is {np.mean(scores)}')\n",
    "print(f'Maximum accuracy is {max_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing age group: O\n",
      "Best parameters for age group O: {'max_depth': 3, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Average accuracy for age O is 0.6746666666666666\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "def extract_age(name):\n",
    "    return name[0]\n",
    "\n",
    "# Assume names_k8 and flattened_k8 are already defined\n",
    "ages_k14 = [extract_age(name) for name in names_k14]\n",
    "X = np.array(flattened_k14)\n",
    "y = np.array(ages_k14)\n",
    "unique_ages = np.unique(y)\n",
    "ypoints, xpoints = 110, 300  # Set the appropriate image dimensions\n",
    "\n",
    "# Number of trials for train-test splitting\n",
    "num_trials = 50\n",
    "age = unique_ages[0]\n",
    "print(f\"Processing age group: {age}\")\n",
    "\n",
    "# Binary classification: 1 if the sample belongs to the current age, 0 otherwise\n",
    "binary_labels = np.where(y == age, 1, 0)\n",
    "\n",
    "# Parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 500, 1000],   # Number of trees in the forest\n",
    "    'max_depth': [3, 5, 7, 20, None],             # Tree depth (None means no limit)\n",
    "    'min_samples_split': [2, 3, 5, 10],           # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform Grid Search to get the best parameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=40),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X, binary_labels)\n",
    "\n",
    "# Extract best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters for age group {age}: {best_params}\")\n",
    "\n",
    "scores = []\n",
    "# Loop for multiple random splits\n",
    "for trial in range(num_trials):\n",
    "    # Randomize train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, binary_labels, test_size=0.2, random_state=40+trial)\n",
    "\n",
    "    # Initialize and train the Random Forest classifier using best_params\n",
    "    rf_classifier = RandomForestClassifier(random_state=40, **best_params)\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    score = rf_classifier.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "    \n",
    "print(f'Average accuracy for age {age} is {np.mean(scores)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing age group: O\n",
      "Params: n_estimators=50, max_depth=3, min_samples_split=2, min_samples_leaf=1, avg_accuracy=0.6773, Best so far: 0.6773\n",
      "Params: n_estimators=50, max_depth=3, min_samples_split=2, min_samples_leaf=2, avg_accuracy=0.6813, Best so far: 0.6813\n",
      "Params: n_estimators=50, max_depth=3, min_samples_split=2, min_samples_leaf=4, avg_accuracy=0.6853, Best so far: 0.6853\n",
      "Params: n_estimators=50, max_depth=3, min_samples_split=3, min_samples_leaf=1, avg_accuracy=0.6733, Best so far: 0.6853\n",
      "Params: n_estimators=50, max_depth=3, min_samples_split=3, min_samples_leaf=2, avg_accuracy=0.6813, Best so far: 0.6853\n",
      "Params: n_estimators=50, max_depth=3, min_samples_split=3, min_samples_leaf=4, avg_accuracy=0.6853, Best so far: 0.6853\n",
      "Params: n_estimators=50, max_depth=3, min_samples_split=5, min_samples_leaf=1, avg_accuracy=0.6760, Best so far: 0.6853\n",
      "Params: n_estimators=50, max_depth=3, min_samples_split=5, min_samples_leaf=2, avg_accuracy=0.6733, Best so far: 0.6853\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Initialize and train the Random Forest classifier with current parameters\u001b[39;00m\n\u001b[1;32m     50\u001b[0m rf_classifier \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[1;32m     51\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[1;32m     52\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m\n\u001b[1;32m     56\u001b[0m )\n\u001b[0;32m---> 57\u001b[0m \u001b[43mrf_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m score \u001b[38;5;241m=\u001b[39m rf_classifier\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n\u001b[1;32m     59\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_forest.py:478\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[0;32m--> 478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    508\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_forest.py:479\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 479\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mappend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[1;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[1;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[1;32m    508\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_base.py:145\u001b[0m, in \u001b[0;36mBaseEnsemble._make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_estimator\u001b[39m(\u001b[38;5;28mself\u001b[39m, append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    140\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make and configure a copy of the `estimator_` attribute.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    Warning: This method should be used to properly instantiate new\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    sub-estimators.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{p: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_params})\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:90\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct a new unfitted estimator with the same parameters.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mClone does a deep copy of the model in an estimator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03mFalse\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_clone__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(estimator):\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_clone__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _clone_parametrized(estimator, safe\u001b[38;5;241m=\u001b[39msafe)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:296\u001b[0m, in \u001b[0;36mBaseEstimator.__sklearn_clone__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sklearn_clone__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:131\u001b[0m, in \u001b[0;36m_clone_parametrized\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m params_set \u001b[38;5;241m=\u001b[39m \u001b[43mnew_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# quick sanity check of the parameters of the clone\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m new_object_params:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:243\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03mGet parameters for this estimator.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    Parameter names mapped to their values.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    242\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_param_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    244\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:208\u001b[0m, in \u001b[0;36mBaseEstimator._get_param_names\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# introspect the constructor arguments to find the model parameters\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# to represent\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m init_signature \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Consider the constructor parameters excluding 'self'\u001b[39;00m\n\u001b[1;32m    210\u001b[0m parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    211\u001b[0m     p\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m init_signature\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m p\u001b[38;5;241m.\u001b[39mVAR_KEYWORD\n\u001b[1;32m    214\u001b[0m ]\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/inspect.py:3130\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   3128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   3129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_wrapped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/inspect.py:2879\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   2876\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   2877\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   2878\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigcls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2880\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfollow_wrapper_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_wrapped\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/inspect.py:2330\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mreplace(parameters\u001b[38;5;241m=\u001b[39mnew_params)\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[1;32m   2328\u001b[0m     \u001b[38;5;66;03m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[1;32m   2329\u001b[0m     \u001b[38;5;66;03m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[0;32m-> 2330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mskip_bound_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_bound_arg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[1;32m   2334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[1;32m   2335\u001b[0m                                    skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg)\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/inspect.py:2154\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Private helper: constructs Signature for the given python function.\"\"\"\u001b[39;00m\n\u001b[1;32m   2153\u001b[0m is_duck_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43misfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _signature_is_functionlike(func):\n\u001b[1;32m   2156\u001b[0m         is_duck_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 500, 1000, 5000],       # Number of trees in the forest\n",
    "    'max_depth': [3, 5, 7, 20, None],                      # Tree depth (None means no limit)\n",
    "    'min_samples_split': [2, 3, 5, 10],                    # Minimum samples required to split a node\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "def extract_age(name):\n",
    "    return name[0]\n",
    "\n",
    "# Assuming names_k14 and flattened_k14 are already defined\n",
    "ages_k14 = [extract_age(name) for name in names_k14]\n",
    "X = np.array(flattened_k14)\n",
    "y = np.array(ages_k14)\n",
    "unique_ages = np.unique(y)\n",
    "ypoints, xpoints = 110, 300  # Set the appropriate image dimensions\n",
    "\n",
    "# Number of trials for train-test splitting\n",
    "num_trials = 50\n",
    "\n",
    "for age in unique_ages:\n",
    "    print(f\"\\nProcessing age group: {age}\")\n",
    "    # Binary classification: 1 if the sample belongs to the current age, 0 otherwise\n",
    "    binary_labels = np.where(y == age, 1, 0)\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    # Loop over every combination of parameters\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for min_samples_split in param_grid['min_samples_split']:\n",
    "                for min_samples_leaf in param_grid['min_samples_leaf']:\n",
    "                    scores = []\n",
    "                    # Loop for multiple random splits\n",
    "                    for trial in range(num_trials):\n",
    "                        # Randomize train-test split\n",
    "                        X_train, X_test, y_train, y_test = train_test_split(\n",
    "                            X, binary_labels, test_size=0.2, random_state=42 + trial\n",
    "                        )\n",
    "                        \n",
    "                        # Initialize and train the Random Forest classifier with current parameters\n",
    "                        rf_classifier = RandomForestClassifier(\n",
    "                            n_estimators=n_estimators,\n",
    "                            max_depth=max_depth,\n",
    "                            min_samples_split=min_samples_split,\n",
    "                            min_samples_leaf=min_samples_leaf,\n",
    "                            random_state=40\n",
    "                        )\n",
    "                        rf_classifier.fit(X_train, y_train)\n",
    "                        score = rf_classifier.score(X_test, y_test)\n",
    "                        scores.append(score)\n",
    "                    \n",
    "                    avg_score = np.mean(scores)\n",
    "                    \n",
    "                    # Update best score and parameters if current avg_score is better\n",
    "                    if avg_score > best_score:\n",
    "                        best_score = avg_score\n",
    "                        best_params = {\n",
    "                            'n_estimators': n_estimators,\n",
    "                            'max_depth': max_depth,\n",
    "                            'min_samples_split': min_samples_split,\n",
    "                            'min_samples_leaf': min_samples_leaf\n",
    "                        }\n",
    "                    \n",
    "                    # Print best accuracy so far after evaluating this set of parameters\n",
    "                    print(f\"Params: n_estimators={n_estimators}, max_depth={max_depth}, \"\n",
    "                          f\"min_samples_split={min_samples_split}, min_samples_leaf={min_samples_leaf}, \"\n",
    "                          f\"avg_accuracy={avg_score:.4f}, Best so far: {best_score:.4f}\")\n",
    "    \n",
    "    # Print best parameter set for the current age group\n",
    "    print(f\"\\nBest parameters for age {age}: {best_params} with accuracy {best_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
